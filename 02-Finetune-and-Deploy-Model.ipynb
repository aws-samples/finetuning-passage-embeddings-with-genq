{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Finetune Passage Embeddings with GenQ\n",
    "\n",
    "## Part 2: Finetune and Deploy the Embedding Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%writefile scripts/requirements.txt\n",
    "\n",
    "sentence-transformers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%writefile scripts/train.py\n",
    "\n",
    "from sentence_transformers import InputExample, datasets, models, SentenceTransformer, losses\n",
    "import boto3\n",
    "import logging\n",
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1)\n",
    "    parser.add_argument(\"--train_batch_size\", type=int, default=12)\n",
    "    parser.add_argument(\"--model_name\", type=str, default='distilbert-base-uncased')\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--bucket\", type=str)\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    parser.add_argument(\"--training_dir\", type=str, default=os.environ[\"SM_HP_TRAINING_DIR\"])\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # Set up logging\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.getLevelName(\"INFO\"),\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    )\n",
    "\n",
    "    logger.info(\"loading dataset from s3\")\n",
    "\n",
    "    # load datasets\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    obj_keys = [obj['Key'] for obj in s3_client.list_objects_v2(Bucket=args.bucket, Prefix=args.training_dir)['Contents']]\n",
    "    pairs = []\n",
    "    for key in obj_keys:\n",
    "        obj = s3_client.get_object(Bucket=args.bucket, Key=key)['Body'].read().decode('utf-8')\n",
    "        lines = obj.split('\\n')\n",
    "        for line in lines:\n",
    "            if '\\t' not in line:\n",
    "                continue\n",
    "            else:\n",
    "                q, p = line.split('\\t')\n",
    "                pairs.append(InputExample(\n",
    "                    texts=[q, p]\n",
    "                ))\n",
    "\n",
    "    logger.info(f\"done. {len(pairs)} pairs loaded.\")\n",
    "\n",
    "    batch_size = args.train_batch_size\n",
    "\n",
    "    loader = datasets.NoDuplicatesDataLoader(\n",
    "        pairs, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    logger.info(f\"loading model: {args.model_name}\")\n",
    "\n",
    "    base = models.Transformer(args.model_name)\n",
    "    pooler = models.Pooling(\n",
    "        base.get_word_embedding_dimension(),\n",
    "        pooling_mode_mean_tokens=True\n",
    "    )\n",
    "\n",
    "    model = SentenceTransformer(modules=[base, pooler])\n",
    "\n",
    "    epochs = args.epochs\n",
    "    warmup_steps = int(len(loader) * epochs * 0.1)\n",
    "\n",
    "    loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "    model.fit(\n",
    "        train_objectives=[(loader, loss)],\n",
    "        epochs=epochs,\n",
    "        warmup_steps=warmup_steps,\n",
    "        output_path=f's3://{args.bucket}/{args.model_dir}',\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    model.save(args.model_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "import sagemaker\n",
    "from datetime import datetime\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "prefix = \"pubmed-finetuning\"\n",
    "\n",
    "# hyperparameters which are passed to the training job\n",
    "hyperparameters={\n",
    "    'epochs': 1,\n",
    "    'train_batch_size': 24,\n",
    "    'model_name': 'distilbert-base-uncased',\n",
    "    'bucket': bucket,\n",
    "    'training_dir': f'{prefix}/data/training'\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "        entry_point='train.py',\n",
    "        source_dir='./scripts',\n",
    "        instance_type='ml.p3.2xlarge',\n",
    "        instance_count=1,\n",
    "        role=role,\n",
    "        transformers_version='4.26',\n",
    "        pytorch_version='1.13',\n",
    "        py_version='py39',\n",
    "        hyperparameters=hyperparameters\n",
    ")\n",
    "\n",
    "training_job_name = f\"distilbert-finetuned-pubmed-{datetime.utcnow().isoformat().replace(':', '-').replace('.', '-')}\"\n",
    "\n",
    "huggingface_estimator.fit({\n",
    "        'train': f's3://{bucket}/{prefix}/data/training'\n",
    "    },\n",
    "    job_name=training_job_name\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!mkdir distilbert-finetuned-pubmed\n",
    "!aws s3 cp {huggingface_estimator.model_data} . && tar -xf model.tar.gz -C distilbert-finetuned-pubmed\n",
    "!ls distilbert-finetuned-pubmed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sentence_transformers import models, SentenceTransformer\n",
    "\n",
    "bert = models.Transformer('distilbert-base-uncased')\n",
    "pooler = models.Pooling(\n",
    "    bert.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=True\n",
    ")\n",
    "\n",
    "model = SentenceTransformer(modules=[bert, pooler])\n",
    "model.save('distilbert-embedder')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bonus: Deploy Finetuned Model to Sagemaker Endpoint"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%writefile scripts/inference.py\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Helper: Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "  # Load model from HuggingFace Hub\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "  model = AutoModel.from_pretrained(model_dir)\n",
    "  return model, tokenizer\n",
    "\n",
    "def predict_fn(data, model_and_tokenizer):\n",
    "    # destruct model and tokenizer\n",
    "    model, tokenizer = model_and_tokenizer\n",
    "\n",
    "    # Tokenize sentences\n",
    "    sentences = data.pop(\"inputs\", data)\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    # return dictonary, which will be json serializable\n",
    "    return {\"vectors\": sentence_embeddings.tolist()}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "finetuned_model = HuggingFaceModel(\n",
    "    model_data=f's3://{bucket}/{training_job_name}/output/model.tar.gz',\n",
    "    source_dir='./scripts',\n",
    "    entry_point='inference.py',\n",
    "    transformers_version='4.26',\n",
    "    pytorch_version='1.13',\n",
    "    py_version='py39',\n",
    "    role=role\n",
    ")\n",
    "\n",
    "finetuned_predictor = finetuned_model.deploy(\n",
    "   initial_instance_count=1,\n",
    "   instance_type=\"ml.g4dn.xlarge\"\n",
    ")\n",
    "\n",
    "# finetuned_predictor.delete_endpoint()\n",
    "\n",
    "with open('.endpoint_name', 'w') as f:\n",
    "    f.write(finetuned_predictor.endpoint_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

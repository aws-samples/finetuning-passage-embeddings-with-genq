{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Passage Embeddings with GenQ\n",
    "\n",
    "## Part 2: Fine-tune and Deploy the Embedding Model\n",
    "\n",
    "In Part 1, we used a generative model to create synthetic queries for each of our document passages. In this notebook, we will use a [Hugging Face Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-estimator) to finetune our generic embedding model.\n",
    "\n",
    "## 2.1. Create the custom `train.py` script\n",
    "\n",
    "Our training script will use the [Sentence-Transformers](https://www.sbert.net/) library, which integrates well with Hugging Face and simplifies the use of embedding models. We can ensure that the `sentence-transformers` package is installed in the Hugging Face DLC by adding a `requirements.txt` to our source directory.\n",
    "\n",
    "In this case, we will finetune [`distilbert-base-uncased`](https://huggingface.co/distilbert-base-uncased), which is a lightweight version of the BERT base model. To use the model for sentence embeddings instead of the default masked language modeling task, we simply need to add a mean pooling layer to the output.\n",
    "\n",
    "The other key component of the `train.py` script below is the [`MultipleNegativesRankingLoss`](https://www.sbert.net/docs/package_reference/losses.html#multiplenegativesrankingloss) loss function. Each iteration, we give the model a batch of (query, passage) pairs and have it attempt to make the appropriate associations. Larger batch sizes force the model to be more discriminative. Note that we also have to use the [`NoDuplicatesDataLoader`](https://www.sbert.net/docs/package_reference/datasets.html#noduplicatesdataloader) with this loss function to ensure that we don't end up with multiple queries from the same passage in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/requirements.txt\n",
    "\n",
    "sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/train.py\n",
    "\n",
    "from sentence_transformers import InputExample, datasets, models, SentenceTransformer, losses\n",
    "import boto3\n",
    "import logging\n",
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument('--epochs', type=int, default=1)\n",
    "    parser.add_argument('--train_batch_size', type=int, default=12)\n",
    "    parser.add_argument('--model_name', type=str, default='distilbert-base-uncased')\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument('--bucket', type=str)\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--training_dir', type=str, default=os.environ['SM_HP_TRAINING_DIR'])\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # Set up logging\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.getLevelName('INFO'),\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    )\n",
    "\n",
    "    logger.info('loading dataset from s3')\n",
    "\n",
    "    # load datasets\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    obj_keys = [obj['Key'] for obj in s3_client.list_objects_v2(Bucket=args.bucket, Prefix=args.training_dir)['Contents']]\n",
    "    pairs = []\n",
    "    for key in obj_keys:\n",
    "        obj = s3_client.get_object(Bucket=args.bucket, Key=key)['Body'].read().decode('utf-8')\n",
    "        lines = obj.split('\\n')\n",
    "        for line in lines:\n",
    "            if '\\t' not in line:\n",
    "                continue\n",
    "            else:\n",
    "                q, p = line.split('\\t')\n",
    "                pairs.append(InputExample(\n",
    "                    texts=[q, p]\n",
    "                ))\n",
    "\n",
    "    logger.info(f'done. {len(pairs)} pairs loaded.')\n",
    "\n",
    "    batch_size = args.train_batch_size\n",
    "\n",
    "    loader = datasets.NoDuplicatesDataLoader(\n",
    "        pairs, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    logger.info(f'loading model: {args.model_name}')\n",
    "\n",
    "    base = models.Transformer(args.model_name)\n",
    "    pooler = models.Pooling(\n",
    "        base.get_word_embedding_dimension(),\n",
    "        pooling_mode_mean_tokens=True\n",
    "    )\n",
    "\n",
    "    model = SentenceTransformer(modules=[base, pooler])\n",
    "\n",
    "    epochs = args.epochs\n",
    "    warmup_steps = int(len(loader) * epochs * 0.1)\n",
    "\n",
    "    loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "    model.fit(\n",
    "        train_objectives=[(loader, loss)],\n",
    "        epochs=epochs,\n",
    "        warmup_steps=warmup_steps,\n",
    "        output_path=f's3://{args.bucket}/{args.model_dir}',\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    model.save(args.model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize and launch the training job**\n",
    "\n",
    "Next, we use a [`HuggingFace`](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-estimator) Estimator to configure our fine-tuning job with our custom entrypoint and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "import sagemaker\n",
    "from datetime import datetime\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "prefix = 'pubmed-finetuning'\n",
    "\n",
    "hyperparameters = {\n",
    "    'epochs': 1,\n",
    "    'train_batch_size': 24,\n",
    "    'model_name': 'distilbert-base-uncased',\n",
    "    'bucket': bucket,\n",
    "    'training_dir': f'{prefix}/data/training'\n",
    "}\n",
    "\n",
    "huggingface_estimator = HuggingFace(\n",
    "        entry_point='train.py',\n",
    "        source_dir='./scripts',\n",
    "        instance_type='ml.p3.2xlarge',\n",
    "        instance_count=1,\n",
    "        role=role,\n",
    "        transformers_version='4.26',\n",
    "        pytorch_version='1.13',\n",
    "        py_version='py39',\n",
    "        hyperparameters=hyperparameters\n",
    ")\n",
    "\n",
    "training_job_name = f\"distilbert-finetuned-pubmed-{datetime.utcnow().isoformat().replace(':', '-').replace('.', '-')}\"\n",
    "\n",
    "huggingface_estimator.fit({\n",
    "        'train': f's3://{bucket}/{prefix}/data/training'\n",
    "    },\n",
    "    job_name=training_job_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download model artifacts for finetuned and baseline models**\n",
    "\n",
    "The output of a Hugging Face training job is a `model.tar.gz` artifact, which contains the model weights and configuration. We will copy it to our notebook and decompress it for local evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘distilbert-finetuned-pubmed’: File exists\n",
      "download: s3://sagemaker-us-east-1-352937523354/distilbert-finetuned-pubmed-2023-05-31T13-01-30-427842/output/model.tar.gz to ./model.tar.gz\n",
      "1_Pooling\t\t\t   sentence_bert_config.json\n",
      "config.json\t\t\t   special_tokens_map.json\n",
      "config_sentence_transformers.json  tokenizer_config.json\n",
      "modules.json\t\t\t   tokenizer.json\n",
      "pytorch_model.bin\t\t   vocab.txt\n",
      "README.md\n"
     ]
    }
   ],
   "source": [
    "!mkdir distilbert-finetuned-pubmed\n",
    "!aws s3 cp {huggingface_estimator.model_data} . && tar -xf model.tar.gz -C distilbert-finetuned-pubmed\n",
    "!ls distilbert-finetuned-pubmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, we will also download the base model directly from Hugging Face. Don't forget to add the pooling layer to use it for sentence embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import models, SentenceTransformer\n",
    "\n",
    "bert = models.Transformer('distilbert-base-uncased')\n",
    "pooler = models.Pooling(\n",
    "    bert.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=True\n",
    ")\n",
    "\n",
    "model = SentenceTransformer(modules=[bert, pooler])\n",
    "model.save('distilbert-embedder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Deploy Finetuned Model to Sagemaker Endpoint\n",
    "\n",
    "You can also deploy the fine-tuned model to a SageMaker Endpoint for simplified, scalable inferencing. In this case, you will need to override the default `inference.py` that comes in the Hugging Face DLC to use the [`SentenceTransformer`](https://www.sbert.net/docs/package_reference/SentenceTransformer.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/inference.py\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    model = SentenceTransformer(model_dir)\n",
    "    return model, model.tokenizer\n",
    "\n",
    "\n",
    "def predict_fn(data, model_and_tokenizer):\n",
    "    # destruct model and tokenizer\n",
    "    model, tokenizer = model_and_tokenizer\n",
    "\n",
    "    # Tokenize sentences\n",
    "    sentences = data.pop('inputs', data)\n",
    "    \n",
    "    sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "    # return dictionary, which will be json serializable\n",
    "    return {'vectors': sentence_embeddings.tolist()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "finetuned_model = HuggingFaceModel(\n",
    "    model_data=f's3://{bucket}/{training_job_name}/output/model.tar.gz',\n",
    "    source_dir='./scripts',\n",
    "    entry_point='inference.py',\n",
    "    transformers_version='4.26',\n",
    "    pytorch_version='1.13',\n",
    "    py_version='py39',\n",
    "    role=role\n",
    ")\n",
    "\n",
    "finetuned_predictor = finetuned_model.deploy(\n",
    "   initial_instance_count=1,\n",
    "   instance_type='ml.g4dn.xlarge'\n",
    ")\n",
    "\n",
    "# finetuned_predictor.delete_endpoint()\n",
    "\n",
    "with open('.endpoint_name', 'w') as f:\n",
    "    f.write(finetuned_predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_predictor.predict({\n",
    "    'inputs': ['this is a test sentence', 'this is another sentence']\n",
    "})\n",
    "\n",
    "# Response is a dict with 'vectors' value that is a list of embedding vectors, one per input sentence, e.g.:\n",
    "# {'vectors': [[0.2859458327293396, ...]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

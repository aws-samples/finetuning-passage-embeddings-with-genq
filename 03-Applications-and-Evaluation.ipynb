{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finetune Passage Embeddings with GenQ\n",
    "\n",
    "\n",
    "## Part 3: Applications and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create vectorstores for both models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "with open('data/metadatas.json', 'r') as f:\n",
    "    metadatas = json.load(f)\n",
    "\n",
    "with open('data/passages.json', 'r') as f:\n",
    "    passages = json.load(f)\n",
    "\n",
    "embeddings_genq = HuggingFaceEmbeddings(model_name='distilbert-finetuned-pubmed')\n",
    "embeddings_base = HuggingFaceEmbeddings(model_name='distilbert-embedder')\n",
    "\n",
    "print(\"Creating vector store for base model...\")\n",
    "db_base = Chroma.from_texts(passages, embeddings_base, metadatas=metadatas, persist_directory='db_base_pubmed')\n",
    "\n",
    "print(\"Creating vector store for finetuned model...\")\n",
    "db_genq = Chroma.from_texts(passages, embeddings_genq, metadatas=metadatas, persist_directory='db_genq_pubmed')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Compare the retrieval results for a couple example queries**\n",
    "\n",
    "The base model's results are of little use on this domain-specific embedding task. Moreover, it was not finetuned to associate (query, passage) pairs. However, a qualitative inspection of the GenQ model shows extremely promising results. For the more abstract query about the relationship between environment and health, it draws relevant results from multiple articles. For the more specific question about myriocin, all of the top results come from the same source."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"relationship between environment and health\",\n",
    "    \"what is myriocin useful for?\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"Query: {q}\\n\")\n",
    "    print(f\"Base Model: {db_base.similarity_search(q)}\\n\")\n",
    "    print(f\"GenQ Model: {db_genq.similarity_search(q)}\")\n",
    "    print(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Retrieval Augmented Generation\n",
    "\n",
    "Retrieval Augmented Generation is a form of question answering in which a large language model (such as OpenAI's gpt3.5-turbo) scopes its response to relevant content returned by our passage retrievers. RAG is useful in cases where you want to limit the response to your own dataset in an efficient manner. If the LLM deems that the presented context does not adequately answer the question, it will respond with `I don't know.`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = getpass('OpenAI API Key: ')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain import OpenAI\n",
    "\n",
    "query = \"what is myriocin successful in treating\"\n",
    "\n",
    "chain_base = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    OpenAI(temperature=0),\n",
    "    chain_type=\"map_reduce\",\n",
    "    retriever=db_base.as_retriever()\n",
    ")\n",
    "\n",
    "print(\"Base Model:\")\n",
    "print(chain_base({\"question\": query}))\n",
    "\n",
    "chain_genq = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    OpenAI(temperature=0),\n",
    "    chain_type=\"map_reduce\",\n",
    "    retriever=db_genq.as_retriever()\n",
    ")\n",
    "\n",
    "print(\"Finetuned Model:\")\n",
    "print(chain_genq({\"question\": query}))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the base embedding model, the retrieved documents are not of much use, so the LLM responds `I don't know.` However, the finetuned model gives a concise, accurate response based on the effective passage retrieval. It even returns the source document so that users can dive deeper.\n",
    "\n",
    "## Extractive Question Answering\n",
    "\n",
    "Extractive Question Answering is another form of QA in which we use models specifically tuned to pull the relevant answer from the passage. This is useful because there is no generative element - the results are directly from the source material. However, it can help highlight relevant information and sort results based on relevance, all without relying on an external LLM API. In this instance, we will deploy the extractive QA model to a Sagemaker endpoint in our account to keep full control of our requests."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Extractive Question Answering\n",
    "\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "hub = {\n",
    "    'HF_MODEL_ID':'deepset/roberta-base-squad2',\n",
    "    'HF_TASK':'question-answering'\n",
    "}\n",
    "\n",
    "transformers_version = '4.26.0'\n",
    "pytorch_version = '1.13.1'\n",
    "py_version = 'py39'\n",
    "use_gpu = True\n",
    "\n",
    "# https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-inference-containers\n",
    "image_uri = f\"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:{pytorch_version}-transformers{transformers_version}-{'gpu' if use_gpu else 'cpu'}-{py_version}{'-cu117' if use_gpu else ''}-ubuntu20.04\"\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "qa_model = HuggingFaceModel(\n",
    "    image_uri=image_uri,\n",
    "    env=hub,\n",
    "    role=role\n",
    ")\n",
    "\n",
    "qa_predictor = qa_model.deploy(\n",
    "    initial_instance_count=1, # number of instances\n",
    "    instance_type='ml.g4dn.xlarge' # ec2 instance type\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "query = \"what cancers are most common in elderly patients\"\n",
    "\n",
    "docs = db_genq.similarity_search(query)\n",
    "\n",
    "display(HTML(f\"<h3>Query: {query}</h3>\"))\n",
    "\n",
    "answers = []\n",
    "for doc in docs:\n",
    "    context = doc.page_content\n",
    "    answers.append(qa_predictor.predict({\n",
    "        \"inputs\": {\n",
    "            \"question\": query,\n",
    "            \"context\": context\n",
    "        }\n",
    "    }))\n",
    "    answers[-1]['context'] = context\n",
    "\n",
    "answers = sorted(answers, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "# return results sorted by relevance\n",
    "for ans in answers:\n",
    "    context = ans['context']\n",
    "    display(HTML(f\"{context[:ans['start']]}<b>{context[ans['start']:ans['end']]}</b>{context[ans['end']:]}<br><i>Relevance: {round(ans['score'], 2)}</i>\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualize Embedding Space"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "import textwrap\n",
    "import plotly.offline as pyo\n",
    "\n",
    "\n",
    "def prepare_embedding_data(db: Chroma, query: str, limit: int = 1000):\n",
    "    df = pd.DataFrame(db._collection.get(limit=limit, include=[\"documents\", \"embeddings\", \"metadatas\"]))\n",
    "    df[\"returned_doc\"] = False\n",
    "    df[\"size\"] = 7.5\n",
    "\n",
    "    query_emb = db._embedding_function.embed_query(query)\n",
    "    res = db._collection.query(query_embeddings=query_emb, include=[\"documents\", \"embeddings\", \"metadatas\"])\n",
    "    df_res = pd.DataFrame([{\"ids\": res[\"ids\"][0][i], \"embeddings\": res[\"embeddings\"][0][i], \"metadatas\": res[\"metadatas\"][0][i], \"documents\": res[\"documents\"][0][i]} for i in range(len(res[\"embeddings\"][0]))])\n",
    "    df_res[\"returned_doc\"] = True\n",
    "    df_res[\"size\"] = 20\n",
    "\n",
    "    df = pd.concat([df, df_res])\n",
    "\n",
    "    df[\"source\"] = df[\"metadatas\"].apply(lambda x: x[\"source\"])\n",
    "    df[\"documents\"] = df[\"documents\"].apply(lambda x: \"<br>\".join(textwrap.wrap(x)))\n",
    "\n",
    "    X = np.array([e for e in df[\"embeddings\"]])\n",
    "\n",
    "    pca = PCA(2)\n",
    "    X_out = pca.fit_transform(X)\n",
    "\n",
    "    df[\"x0\"] = X_out[:,0]\n",
    "    df[\"x1\"] = X_out[:,1]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_figure(df: pd.DataFrame, title: str):\n",
    "    fig = px.scatter(df, x=\"x0\", y=\"x1\", color=\"returned_doc\", custom_data=[\"documents\"], size=\"size\", title=title)\n",
    "    fig.update_traces(\n",
    "        hovertemplate=\"%{customdata[0]}\"\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "query = \"relationship between environment and health\"\n",
    "\n",
    "df_base = prepare_embedding_data(db_base, query=query)\n",
    "df_genq = prepare_embedding_data(db_genq, query=query)\n",
    "\n",
    "fig_base = create_figure(df_base, title=f\"**Base Model**: {query}\")\n",
    "fig_genq = create_figure(df_genq, title=f\"**GenQ Model**: {query}\")\n",
    "\n",
    "pyo.init_notebook_mode()\n",
    "pyo.iplot(fig_genq)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pyo.iplot(fig_base)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "import json\n",
    "\n",
    "\n",
    "class ContentHandler(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, inputs: list[str], model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": inputs, **model_kwargs})\n",
    "        return input_str.encode('utf-8')\n",
    "\n",
    "    def transform_output(self, output: bytes) -> List[List[float]]:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"vectors\"]\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "with open('.endpoint_name', 'r') as f:\n",
    "    endpoint_name = f.read()\n",
    "\n",
    "embeddings = SagemakerEndpointEmbeddings(\n",
    "    endpoint_name=endpoint_name,\n",
    "    region_name=\"us-east-1\",\n",
    "    content_handler=content_handler\n",
    ")\n",
    "\n",
    "# now you can use this SagemakerEndpointEmbeddings object in a vectorstore as shown above\n",
    "db = Chroma.from_texts(passages, embeddings, metadatas=metadatas)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

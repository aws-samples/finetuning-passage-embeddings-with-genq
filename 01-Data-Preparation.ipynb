{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Passage Embeddings with GenQ\n",
    "\n",
    "State-of-the-art semantic search applications like [extractive question answering](https://docs.pinecone.io/docs/extractive-question-answering) and [Retrieval Augmented Generation](https://arxiv.org/abs/2005.11401) (RAG) have gained significant attention recently, allowing users to query a large document corpus using natural language questions. One of the most important components in these systems is the **embedding model**, which maps input text into a vector, providing a numeric representation of the semantic meaning of the text. An effective model is tuned to place passages with similar meaning in similar locations in the embedding space. In a process known as **retrieval**, this space is efficiently interrogated to find the most relevant documents given an input query.\n",
    "\n",
    "In practice, general-purpose language models do not perform well on this retrieval task, particularly when dealing with documents related to a specialized domain, like medicine or law. The typical solution is to \"fine-tune\" a generic model to the particular task and field of interest by feeding it thousands of labeled training examples. For a semantic search engine, the most valuable dataset would be human-generated (query, passage) pairs.\n",
    "\n",
    "For example, this excerpt from the [Sagemaker Developer Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/train-model.html) contains an appropriate answer for the following query:\n",
    "\n",
    "```\n",
    "Query: What instance type should I use for an NLP model?\n",
    "\n",
    "Relevant Passage: Choose the right instance type and infrastructure management tools for your use case. You can start from a small instance and scale up depending on your workload. For training a model on a tabular dataset, start with the smallest CPU instance of the C4 or C5 instance families. For training a large model for computer vision or natural language processing, start with the smallest GPU instance of the P2, P3, G4dn or G5 instance families.\n",
    "```\n",
    "\n",
    "However, it would be extremely expensive and time consuming to have humans generate questions for thousands of documents. In this series of notebooks, we will demonstrate an approach called [**GenQ**](https://www.sbert.net/examples/unsupervised_learning/query_generation/README.html), in which we leverage a separate model to automatically generate so-called \"synthetic\" queries for our dataset. For more details about GenQ, see the [NIPS publication](https://arxiv.org/abs/2104.08663).\n",
    "\n",
    "This demonstration is broken into 3 notebooks which are designed to be completed in order.\n",
    "\n",
    "1. **01-Data-Preparation** (this notebook): initialization, dataset preparation, generating synthetic queries\n",
    "2. 02-Finetune-and-Deploy-Model: finetuning the embedding model and deploying it to a Sagemaker endpoint\n",
    "3. 03-Applications-and-Evaluation: comparing the finetuned model to a baseline and using it for RAG and extractive QA\n",
    "\n",
    "Throughout the demo, we will leverage frameworks like [LangChain](https://python.langchain.com/en/latest/) and [Hugging Face](https://huggingface.co/) to simplify and operationalize the workflow. We will also use [Amazon SageMaker](https://aws.amazon.com/sagemaker/) for fully-managed and scalable inference, training, and deployment.\n",
    "\n",
    "## Part 1: Data Preparation\n",
    "\n",
    "In this notebook, we will download a document dataset, break it into individual passages, and use a GenQ model to generate synthetic queries for each excerpt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Install dependencies\n",
    "\n",
    "**Make sure to use the `conda_pytorch_p39` SageMaker kernel, which comes with many dependencies pre-installed.** We have added a few additional packages to the `requirements.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting datasets==2.12.0\n",
      "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m474.6/474.6 kB\u001B[0m \u001B[31m12.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting langchain==0.0.180\n",
      "  Downloading langchain-0.0.180-py3-none-any.whl (922 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m922.9/922.9 kB\u001B[0m \u001B[31m21.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting openai==0.27.6\n",
      "  Downloading openai-0.27.6-py3-none-any.whl (71 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m71.9/71.9 kB\u001B[0m \u001B[31m21.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting opensearch-py==2.2.0\n",
      "  Downloading opensearch_py-2.2.0-py2.py3-none-any.whl (291 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m291.0/291.0 kB\u001B[0m \u001B[31m8.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting sentence-transformers==2.2.2\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m86.0/86.0 kB\u001B[0m \u001B[31m28.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hCollecting tiktoken==0.4.0\n",
      "  Downloading tiktoken-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m28.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 1)) (21.3)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m212.2/212.2 kB\u001B[0m \u001B[31m53.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: dill<0.3.7,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 1)) (0.3.6)\n",
      "Collecting huggingface-hub<1.0.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m224.5/224.5 kB\u001B[0m \u001B[31m38.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 1)) (3.8.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 1)) (4.63.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 1)) (0.70.14)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 1)) (1.4.4)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 1)) (2022.11.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 1)) (10.0.1)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 1)) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from datasets==2.12.0->-r requirements.txt (line 1)) (1.23.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from langchain==0.0.180->-r requirements.txt (line 2)) (4.0.2)\n",
      "Collecting openapi-schema-pydantic<2.0,>=1.2\n",
      "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m90.0/90.0 kB\u001B[0m \u001B[31m27.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from langchain==0.0.180->-r requirements.txt (line 2)) (8.1.0)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from langchain==0.0.180->-r requirements.txt (line 2)) (1.10.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from langchain==0.0.180->-r requirements.txt (line 2)) (1.4.46)\n",
      "Collecting dataclasses-json<0.6.0,>=0.5.7\n",
      "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
      "Collecting numexpr<3.0.0,>=2.8.4\n",
      "  Downloading numexpr-2.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (380 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m380.7/380.7 kB\u001B[0m \u001B[31m11.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: certifi>=2022.12.07 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from opensearch-py==2.2.0->-r requirements.txt (line 4)) (2022.12.7)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from opensearch-py==2.2.0->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from opensearch-py==2.2.0->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: urllib3<2,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from opensearch-py==2.2.0->-r requirements.txt (line 4)) (1.26.8)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.1/7.1 MB\u001B[0m \u001B[31m53.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: torch>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 5)) (1.13.1)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 5)) (0.14.1)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 5)) (1.0)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 5)) (1.8.1)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 5)) (3.8.1)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.3/1.3 MB\u001B[0m \u001B[31m33.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: regex>=2022.1.18 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from tiktoken==0.4.0->-r requirements.txt (line 6)) (2022.10.31)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 1)) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 1)) (22.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 1)) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 1)) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 1)) (6.0.4)\n",
      "Collecting marshmallow-enum<2.0.0,>=1.5.1\n",
      "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.3.0\n",
      "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.1/49.1 kB\u001B[0m \u001B[31m14.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting typing-inspect>=0.4.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.12.0->-r requirements.txt (line 1)) (4.4.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.12.0->-r requirements.txt (line 1)) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from packaging->datasets==2.12.0->-r requirements.txt (line 1)) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.12.0->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.180->-r requirements.txt (line 2)) (2.0.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.8/7.8 MB\u001B[0m \u001B[31m54.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from nltk->sentence-transformers==2.2.2->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from nltk->sentence-transformers==2.2.2->-r requirements.txt (line 5)) (8.1.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 1)) (2022.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from scikit-learn->sentence-transformers==2.2.2->-r requirements.txt (line 5)) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from torchvision->sentence-transformers==2.2.2->-r requirements.txt (line 5)) (9.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.180->-r requirements.txt (line 2)) (0.4.3)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125925 sha256=d7ec5ed1b27d3426acd428b4eccd0d6f16d86979f2a9a66d2cf70ce21563eb2d\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/4b/68/65/aba8be86302d9988b832f5e1f3417a87e4a868d396e4329f0a\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: tokenizers, sentencepiece, xxhash, typing-inspect, numexpr, tiktoken, responses, opensearch-py, openapi-schema-pydantic, marshmallow, huggingface-hub, transformers, openai, marshmallow-enum, sentence-transformers, datasets, dataclasses-json, langchain\n",
      "  Attempting uninstall: numexpr\n",
      "    Found existing installation: numexpr 2.7.3\n",
      "    Uninstalling numexpr-2.7.3:\n",
      "      Successfully uninstalled numexpr-2.7.3\n",
      "Successfully installed dataclasses-json-0.5.7 datasets-2.12.0 huggingface-hub-0.14.1 langchain-0.0.180 marshmallow-3.19.0 marshmallow-enum-1.5.1 numexpr-2.8.4 openai-0.27.6 openapi-schema-pydantic-1.2.4 opensearch-py-2.2.0 responses-0.18.0 sentence-transformers-2.2.2 sentencepiece-0.1.99 tiktoken-0.4.0 tokenizers-0.13.3 transformers-4.29.2 typing-inspect-0.9.0 xxhash-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Load and preprocess dataset\n",
    "\n",
    "We will use pubmed abstracts as an example of a domain-specific corpus that may not produce great retrieval results with generic embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010486364364624023,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading builder script",
       "rate": null,
       "total": 5129,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44b09b324bb4da49d17fcafb50b9a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.13k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009814977645874023,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading readme",
       "rate": null,
       "total": 2662,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d10e05f5a36943a58a515877acd29707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/2.66k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset pubmed-summarization/document to /home/ec2-user/.cache/huggingface/datasets/ccdv___pubmed-summarization/document/1.0.0/f765ec606c790e8c5694b226814a13f1974ba4ea98280989edaffb152ded5e2b...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0076999664306640625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data",
       "rate": null,
       "total": 779257354,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce12daa7b05a426f875775a64767bbca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/779M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009141683578491211,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data",
       "rate": null,
       "total": 43705498,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41235bd1ad91498397dd343a8ca5eae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/43.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01004338264465332,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data",
       "rate": null,
       "total": 43787908,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86e7772ef904c09a08499c545abdb1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/43.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01093435287475586,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating train split",
       "rate": null,
       "total": 0,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010334968566894531,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating validation split",
       "rate": null,
       "total": 0,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007821798324584961,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating test split",
       "rate": null,
       "total": 0,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset pubmed-summarization downloaded and prepared to /home/ec2-user/.cache/huggingface/datasets/ccdv___pubmed-summarization/document/1.0.0/f765ec606c790e8c5694b226814a13f1974ba4ea98280989edaffb152ded5e2b. Subsequent calls will reuse this data.\n",
      "Number of articles: 2500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "background : the present study was carried out to assess the effects of community nutrition intervention based on advocacy approach on malnutrition status among school - aged children in shiraz , iran.materials and methods : this case - control nutritional intervention has been done between 2008 and 2009 on 2897 primary and secondary school boys and girls ( 7 - 13 years old ) based on advocacy approach in shiraz , iran . \n",
       " the project provided nutritious snacks in public schools over a 2-year period along with advocacy oriented actions in order to implement and promote nutritional intervention . for evaluation of effectiveness of the intervention growth monitoring indices of pre- and post - intervention were statistically compared.results:the frequency of subjects with body mass index lower than 5% decreased significantly after intervention among girls ( p = 0.02 ) . \n",
       " however , there were no significant changes among boys or total population . \n",
       " the mean of all anthropometric indices changed significantly after intervention both among girls and boys as well as in total population . \n",
       " the pre- and post - test education assessment in both groups showed that the student 's average knowledge score has been significantly increased from 12.5  3.2 to 16.8  4.3 ( p < 0.0001).conclusion : this study demonstrates the potential success and scalability of school feeding programs in iran . \n",
       " community nutrition intervention based on the advocacy process model is effective on reducing the prevalence of underweight specifically among female school aged children ."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from IPython.core.display import display, HTML\n",
    "import os\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# load abstracts of pubmed articles\n",
    "dataset = load_dataset('ccdv/pubmed-summarization', 'document', split='train[:2500]')\n",
    "articles = [{'id': str(i).zfill(6), 'abstract': d['abstract']} for i, d in enumerate(dataset)]\n",
    "\n",
    "print(f\"Number of articles: {len(articles)}\")\n",
    "\n",
    "# print an example article\n",
    "display(HTML(articles[0]['abstract']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In semantic search applications, it is common practice to split the input documents into smaller passages. The size of the excerpts requires you to strike a balance between accuracy and specificity - for example, larger passages are more likely to contain a full answer to your query, but they take longer for users (and downstream models) to parse. The most appropriate `chunk_size` will depend on your dataset and processing pipeline. LangChain supports a number of different [Text Splitters](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html) - in this case we will use the [`NLTKTextSplitter`](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/nltk.html), which uses a tokenizer to split input texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "Created a chunk of size 1199, which is longer than the specified 1000\n",
      "Created a chunk of size 1050, which is longer than the specified 1000\n",
      "Created a chunk of size 1346, which is longer than the specified 1000\n",
      "Created a chunk of size 1313, which is longer than the specified 1000\n",
      "Created a chunk of size 1160, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of passages: 20454\n",
      "\n",
      "\n",
      "Example Passages:\n",
      "\n",
      "background : the present study was carried out to assess the effects of community nutrition intervention based on advocacy approach on malnutrition status among school - aged children in shiraz , iran.materials and methods : this case - control nutritional intervention has been done between 2008 and 2009 on 2897 primary and secondary school boys and girls ( 7 - 13 years old ) based on advocacy approach in shiraz , iran .\n",
      "\n",
      "the project provided nutritious snacks in public schools over a 2-year period along with advocacy oriented actions in order to implement and promote nutritional intervention .\n",
      "\n",
      "for evaluation of effectiveness of the intervention growth monitoring indices of pre- and post - intervention were statistically compared.results:the frequency of subjects with body mass index lower than 5% decreased significantly after intervention among girls ( p = 0.02 ) .\n",
      "\n",
      "however , there were no significant changes among boys or total population .\n",
      "\n",
      "however , there were no significant changes among boys or total population .\n",
      "\n",
      "the mean of all anthropometric indices changed significantly after intervention both among girls and boys as well as in total population .\n",
      "\n",
      "the pre- and post - test education assessment in both groups showed that the student 's average knowledge score has been significantly increased from 12.5  3.2 to 16.8  4.3 ( p < 0.0001).conclusion : this study demonstrates the potential success and scalability of school feeding programs in iran .\n",
      "\n",
      "community nutrition intervention based on the advocacy process model is effective on reducing the prevalence of underweight specifically among female school aged children .\n",
      "\n",
      "backgroundanemia in patients with cancer who are undergoing active therapy is commonly encountered and may worsen quality of life in these patients .\n",
      "\n",
      "the effect of blood transfusion is often temporary and may be associated with serious adverse events .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# preprocessing - create passages by splitting articles\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "separator = '\\n\\n'\n",
    "text_splitter = NLTKTextSplitter(separator=separator, chunk_size=1000)\n",
    "\n",
    "passages, metadatas = [], []\n",
    "\n",
    "for a in articles:\n",
    "    split_texts = text_splitter.split_text(a['abstract'])\n",
    "    for p in split_texts:\n",
    "        new_passages = p.split(separator)\n",
    "        passages += new_passages\n",
    "        metadatas += [{'source': a['id']} for _ in new_passages]\n",
    "\n",
    "print(f'Number of passages: {len(passages)}\\n\\n')\n",
    "\n",
    "print('Example Passages:\\n')\n",
    "for p in passages[:10]:\n",
    "    print(p + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Prepare GenQ model and use it to generate potential queries for each passage\n",
    "\n",
    "We will use [`BeIR/query-gen-msmarco-t5-large-v1`](https://huggingface.co/BeIR/query-gen-msmarco-t5-large-v1), which is a T5-based model used for synthetic query generation tasks. The Sagemaker [Hugging Face](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/index.html) integration will greatly simplify this deployment. To define the [`HuggingFaceModel`](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-model), we need to supply an appropriate image URI, environment variables for the model and task, and an appropriate execution role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare (query, passage) pairs with GenQ\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "hub = {\n",
    "    'HF_MODEL_ID':'BeIR/query-gen-msmarco-t5-large-v1',\n",
    "    'HF_TASK':'text2text-generation'\n",
    "}\n",
    "\n",
    "transformers_version = '4.26.0'\n",
    "pytorch_version = '1.13.1'\n",
    "py_version = 'py39'\n",
    "use_gpu = True\n",
    "\n",
    "# https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-inference-containers\n",
    "image_uri = f\"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:{pytorch_version}-transformers{transformers_version}-{'gpu' if use_gpu else 'cpu'}-{py_version}{'-cu117' if use_gpu else ''}-ubuntu20.04\"\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    image_uri=image_uri,\n",
    "    env=hub,\n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the GenQ model has been defined, we can use it to generate (query, passage) pairs for each passage. Because we only need the model to be temporarily deployed for this specific task, it is an excellent candidate for a [SageMaker Transform Job](https://sagemaker.readthedocs.io/en/stable/api/inference/transformer.html). After generating text for our input dataset, the resources will automatically be shut down to limit costs.\n",
    "\n",
    "**Setup SageMaker Batch Transform Job to Generate Queries Asynchronously**\n",
    "\n",
    "At this time, the [Hugging Face Inference DLC only supports the `.jsonl` format](https://huggingface.co/docs/sagemaker/inference#run-batch-transform-with-transformers-and-sagemaker) for batch transforms. We will pass in additional parameters to ensure that 3 concise queries are generated for each passage. The `input.jsonl` file will be copied to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('input.jsonl', 'w') as f:\n",
    "    for p in passages:\n",
    "        f.write(json.dumps({\n",
    "            'inputs': p,\n",
    "            'parameters': {\n",
    "                'max_length': 32,\n",
    "                'num_return_sequences': 3,\n",
    "                'top_p': 0.95,\n",
    "                'do_sample': True\n",
    "            }}) + '\\n')\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "prefix = 'pubmed-finetuning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./input.jsonl to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/batch_input/input.jsonl\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp input.jsonl s3://{bucket}/{prefix}/data/batch_input/input.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [SageMaker Developer Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform-data-processing.html) for more details about the configuration of the transform job. In this case, we use a `join_source` on the input to ensure that we keep the (query, passage) pairs together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# transform job takes about 3 hours with p3.2xlarge\n",
    "\n",
    "batch_job = huggingface_model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    strategy='SingleRecord',\n",
    "    accept='application/json',\n",
    "    assemble_with='Line',\n",
    "    output_path=f's3://{bucket}/{prefix}/data/batch_output'\n",
    ")\n",
    "\n",
    "# starts batch transform job and uses S3 data as input\n",
    "batch_job.transform(\n",
    "    data=f's3://{bucket}/{prefix}/data/batch_input/input.jsonl',\n",
    "    content_type='application/json',\n",
    "    split_type='Line',\n",
    "    join_source='Input'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Optional) Deploy query generation model to SageMaker endpoint**\n",
    "\n",
    "As an alternative to the batch transform job, you can also deploy the model to a SageMaker endpoint using the SDK. This is useful for rapid testing - see an example of the generated queries for this passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Passage:\n",
      "background : the present study was carried out to assess the effects of community nutrition intervention based on advocacy approach on malnutrition status among school - aged children in shiraz , iran.materials and methods : this case - control nutritional intervention has been done between 2008 and 2009 on 2897 primary and secondary school boys and girls ( 7 - 13 years old ) based on advocacy approach in shiraz , iran .\n",
      "\n",
      "Generated Queries:\n",
      "\n",
      "what is nutritional intervention based on advocacy\n",
      "why community nutrition approach\n",
      "what is nutrition advocacy intervention\n"
     ]
    }
   ],
   "source": [
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1, # number of instances\n",
    "    instance_type='ml.g4dn.xlarge', # ec2 instance type\n",
    ")\n",
    "\n",
    "# synchronous inference\n",
    "psg = passages[0]\n",
    "\n",
    "queries = predictor.predict({\n",
    "    'inputs': psg,\n",
    "    'parameters': {\n",
    "        'max_length': 32,\n",
    "        'num_return_sequences': 3,\n",
    "        'top_p': 0.95,\n",
    "        'do_sample': True\n",
    "    }\n",
    "})\n",
    "\n",
    "print(f'\\nPassage:\\n{psg}\\n\\nGenerated Queries:\\n')\n",
    "for q in queries:\n",
    "    print(q['generated_text'])\n",
    "\n",
    "# make sure to delete the endpoint when you're done to limit cost\n",
    "# predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Perform some light transforms on the output to turn into individual (query, passage) pairs\n",
    "\n",
    "Once the queries have been generated, we need to do some light transformations to parse the batch output into a format suitable for the finetuning job. The training pairs will be batched into `.tsv` files containing 1024 samples each. In the next notebook, we will use this dataset to finetune a generic embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "# transform outputs into format suitable for training job\n",
    "local_path = 'batch_output'\n",
    "\n",
    "sagemaker.s3.S3Downloader.download(f'{batch_job.output_path}/input.jsonl.out', local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00679469108581543,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 18773,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12fb3a4dea654041b6530fa47b95b3a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18773 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "training_data_path = 'pubmed_training_pairs'\n",
    "\n",
    "if os.path.isdir(training_data_path) is False:\n",
    "    os.mkdir(training_data_path)\n",
    "\n",
    "pairs = []\n",
    "file_count = 0\n",
    "\n",
    "with open(f'{local_path}/input.jsonl.out', 'r') as f:\n",
    "    for r in tqdm(f.readlines()):\n",
    "        obj = json.loads(r)\n",
    "        psg = obj['inputs'].replace('\\t', ' ').replace('\\n', ' ')\n",
    "\n",
    "        for q in obj['SageMakerOutput']:\n",
    "            pairs.append(q['generated_text'].replace('\\t', ' ') + '\\t' + psg)\n",
    "\n",
    "        if len(pairs) > 1024:\n",
    "            with open(f'{training_data_path}/pairs_{file_count:04}.tsv', 'w', encoding='utf-8') as fp:\n",
    "                fp.write('\\n'.join(pairs))\n",
    "\n",
    "            file_count += 1\n",
    "            pairs = []\n",
    "\n",
    "if pairs is not None:\n",
    "    # save the final, smaller than 1024 batch\n",
    "    with open(f'{training_data_path}/pairs_{file_count:04}.tsv', 'w', encoding='utf-8') as fp:\n",
    "        fp.write('\\n'.join(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: pubmed_training_pairs/pairs_0004.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0004.tsv\n",
      "upload: pubmed_training_pairs/pairs_0006.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0006.tsv\n",
      "upload: pubmed_training_pairs/pairs_0009.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0009.tsv\n",
      "upload: pubmed_training_pairs/pairs_0005.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0005.tsv\n",
      "upload: pubmed_training_pairs/pairs_0001.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0001.tsv\n",
      "upload: pubmed_training_pairs/pairs_0008.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0008.tsv\n",
      "upload: pubmed_training_pairs/pairs_0007.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0007.tsv\n",
      "upload: pubmed_training_pairs/pairs_0012.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0012.tsv\n",
      "upload: pubmed_training_pairs/pairs_0002.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0002.tsv\n",
      "upload: pubmed_training_pairs/pairs_0011.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0011.tsv\n",
      "upload: pubmed_training_pairs/pairs_0000.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0000.tsv\n",
      "upload: pubmed_training_pairs/pairs_0015.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0015.tsv\n",
      "upload: pubmed_training_pairs/pairs_0010.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0010.tsv\n",
      "upload: pubmed_training_pairs/pairs_0003.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0003.tsv\n",
      "upload: pubmed_training_pairs/pairs_0016.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0016.tsv\n",
      "upload: pubmed_training_pairs/pairs_0018.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0018.tsv\n",
      "upload: pubmed_training_pairs/pairs_0013.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0013.tsv\n",
      "upload: pubmed_training_pairs/pairs_0017.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0017.tsv\n",
      "upload: pubmed_training_pairs/pairs_0014.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0014.tsv\n",
      "upload: pubmed_training_pairs/pairs_0019.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0019.tsv\n",
      "upload: pubmed_training_pairs/pairs_0026.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0026.tsv\n",
      "upload: pubmed_training_pairs/pairs_0021.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0021.tsv\n",
      "upload: pubmed_training_pairs/pairs_0024.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0024.tsv\n",
      "upload: pubmed_training_pairs/pairs_0022.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0022.tsv\n",
      "upload: pubmed_training_pairs/pairs_0030.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0030.tsv\n",
      "upload: pubmed_training_pairs/pairs_0027.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0027.tsv\n",
      "upload: pubmed_training_pairs/pairs_0023.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0023.tsv\n",
      "upload: pubmed_training_pairs/pairs_0028.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0028.tsv\n",
      "upload: pubmed_training_pairs/pairs_0032.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0032.tsv\n",
      "upload: pubmed_training_pairs/pairs_0036.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0036.tsv\n",
      "upload: pubmed_training_pairs/pairs_0029.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0029.tsv\n",
      "upload: pubmed_training_pairs/pairs_0031.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0031.tsv\n",
      "upload: pubmed_training_pairs/pairs_0033.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0033.tsv\n",
      "upload: pubmed_training_pairs/pairs_0020.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0020.tsv\n",
      "upload: pubmed_training_pairs/pairs_0025.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0025.tsv\n",
      "upload: pubmed_training_pairs/pairs_0040.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0040.tsv\n",
      "upload: pubmed_training_pairs/pairs_0035.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0035.tsv\n",
      "upload: pubmed_training_pairs/pairs_0034.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0034.tsv\n",
      "upload: pubmed_training_pairs/pairs_0044.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0044.tsv\n",
      "upload: pubmed_training_pairs/pairs_0041.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0041.tsv\n",
      "upload: pubmed_training_pairs/pairs_0037.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0037.tsv\n",
      "upload: pubmed_training_pairs/pairs_0039.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0039.tsv\n",
      "upload: pubmed_training_pairs/pairs_0042.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0042.tsv\n",
      "upload: pubmed_training_pairs/pairs_0043.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0043.tsv\n",
      "upload: pubmed_training_pairs/pairs_0047.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0047.tsv\n",
      "upload: pubmed_training_pairs/pairs_0051.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0051.tsv\n",
      "upload: pubmed_training_pairs/pairs_0050.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0050.tsv\n",
      "upload: pubmed_training_pairs/pairs_0049.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0049.tsv\n",
      "upload: pubmed_training_pairs/pairs_0046.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0046.tsv\n",
      "upload: pubmed_training_pairs/pairs_0038.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0038.tsv\n",
      "upload: pubmed_training_pairs/pairs_0053.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0053.tsv\n",
      "upload: pubmed_training_pairs/pairs_0048.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0048.tsv\n",
      "upload: pubmed_training_pairs/pairs_0045.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0045.tsv\n",
      "upload: pubmed_training_pairs/pairs_0052.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0052.tsv\n",
      "upload: pubmed_training_pairs/pairs_0054.tsv to s3://sagemaker-us-east-1-352937523354/pubmed-finetuning/data/training/pairs_0054.tsv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp pubmed_training_pairs s3://{bucket}/{prefix}/data/training --recursive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

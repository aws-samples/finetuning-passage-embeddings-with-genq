{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Finetuning Passage Embeddings with GenQ\n",
    "\n",
    "State-of-the-art semantic search applications like [extractive question answering](https://docs.pinecone.io/docs/extractive-question-answering) and [Retrieval Augmented Generation](https://arxiv.org/abs/2005.11401) (RAG) have gained significant attention recently, allowing users to query a large document corpus using natural language questions. One of the most important components in these systems is the **embedding model**, which maps input text into a vector, providing a numeric representation of the semantic meaning of the text. An effective model is tuned to place passages with similar meaning in similar locations in the embedding space. In a process known as **retrieval**, this space is efficiently interrogated to find the most relevant documents given an input query.\n",
    "\n",
    "In practice, general-purpose language models do not perform well on this retrieval task, particularly when dealing with documents related to a specialized domain, like medicine or law. The typical solution is to \"fine-tune\" a generic model to the particular task and field of interest by feeding it thousands of labeled training examples. For a semantic search engine, the most valuable dataset would be human-generated (query, passage) pairs.\n",
    "\n",
    "For example, this excerpt from the [Sagemaker Developer Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/train-model.html) contains an appropriate answer for the following query:\n",
    "\n",
    "```\n",
    "Query: What instance type should I use for an NLP model?\n",
    "\n",
    "Relevant Passage: Choose the right instance type and infrastructure management tools for your use case. You can start from a small instance and scale up depending on your workload. For training a model on a tabular dataset, start with the smallest CPU instance of the C4 or C5 instance families. For training a large model for computer vision or natural language processing, start with the smallest GPU instance of the P2, P3, G4dn or G5 instance families.\n",
    "```\n",
    "\n",
    "However, it would be extremely expensive and time consuming to have humans generate questions for thousands of documents. In this series of notebooks, we will demonstrate an approach called [**GenQ**](https://www.sbert.net/examples/unsupervised_learning/query_generation/README.html), in which we leverage a separate model to automatically generate so-called \"synthetic\" queries for our dataset. For more details about GenQ, see the [NIPS publication](https://arxiv.org/abs/2104.08663).\n",
    "\n",
    "This demonstration is broken into 3 notebooks which are designed to be completed in order.\n",
    "\n",
    "1. **01-Data-Preparation** (this notebook): initialization, dataset preparation, generating synthetic queries\n",
    "2. 02-Finetune-and-Deploy-Model: finetuning the embedding model and deploying it to a Sagemaker endpoint\n",
    "3. 03-Applications-and-Evaluation: comparing the finetuned model to a baseline and using it for RAG and extractive QA\n",
    "\n",
    "Throughout the demo, we will leverage frameworks like [LangChain](https://python.langchain.com/en/latest/) and [Hugging Face](https://huggingface.co/) to simplify and operationalize the workflow. We will also use [Amazon SageMaker](https://aws.amazon.com/sagemaker/) for fully-managed and scalable inference, training, and deployment.\n",
    "\n",
    "## Part 1: Data Preparation\n",
    "\n",
    "In this notebook, we will download a document dataset, break it into individual passages, and use a GenQ model to generate synthetic queries for each excerpt."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.0 Install dependencies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1. Load and preprocess dataset\n",
    "\n",
    "We will use pubmed abstracts as an example of a domain-specific corpus that may not produce great retrieval results with generic embedding models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from IPython.core.display import display, HTML\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# load abstracts of pubmed articles\n",
    "dataset = load_dataset(\"ccdv/pubmed-summarization\", \"document\", split=\"train[:2500]\")\n",
    "articles = [{\"id\": str(i).zfill(6), \"abstract\": d[\"abstract\"]} for i, d in enumerate(dataset)]\n",
    "\n",
    "print(f\"Number of articles: {len(articles)}\")\n",
    "\n",
    "# print an example article\n",
    "display(HTML(articles[0]['abstract']))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In semantic search applications, it is common practice to split the input documents into smaller passages. The size of the excerpts requires you to strike a balance between accuracy and specificity - for example, larger passages are more likely to contain a full answer to your query, but they take longer for users (and downstream models) to parse. The most appropriate `chunk_size` will depend on your dataset and processing pipeline. LangChain supports a number of different [Text Splitters](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html) - in this case we will use the [`NLTKTextSplitter`](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/nltk.html), which uses a tokenizer to split input texts."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# preprocessing - create passages by splitting articles\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "separator = '\\n\\n'\n",
    "text_splitter = NLTKTextSplitter(separator=separator, chunk_size=1000)\n",
    "\n",
    "passages, metadatas = [], []\n",
    "\n",
    "for a in articles:\n",
    "    split_texts = text_splitter.split_text(a[\"abstract\"])\n",
    "    for p in split_texts:\n",
    "        new_passages = p.split(separator)\n",
    "        passages += new_passages\n",
    "        metadatas += [{\"source\": a[\"id\"]} for _ in new_passages]\n",
    "\n",
    "print(f\"Number of passages: {len(passages)}\\n\\n\")\n",
    "\n",
    "print(\"Example Passages:\\n\")\n",
    "for p in passages[:10]:\n",
    "    print(p + '\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2. Prepare GenQ model and use it to generate potential queries for each passage\n",
    "\n",
    "We will use [`BeIR/query-gen-msmarco-t5-large-v1`](https://huggingface.co/BeIR/query-gen-msmarco-t5-large-v1), which is a T5-based model used for synthetic query generation tasks. The Sagemaker [Hugging Face](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/index.html) integration will greatly simplify this deployment. To define the [`HuggingFaceModel`](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-model), we need to supply an appropriate image URI, environment variables for the model and task, and an appropriate execution role."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# prepare (query, passage) pairs with GenQ\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "hub = {\n",
    "    'HF_MODEL_ID':'BeIR/query-gen-msmarco-t5-large-v1',\n",
    "    'HF_TASK':'text2text-generation'\n",
    "}\n",
    "\n",
    "transformers_version = '4.26.0'\n",
    "pytorch_version = '1.13.1'\n",
    "py_version = 'py39'\n",
    "use_gpu = True\n",
    "\n",
    "# https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-inference-containers\n",
    "image_uri = f\"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:{pytorch_version}-transformers{transformers_version}-{'gpu' if use_gpu else 'cpu'}-{py_version}{'-cu117' if use_gpu else ''}-ubuntu20.04\"\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    image_uri=image_uri,\n",
    "    env=hub,\n",
    "    role=role\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once deployed, we can take a look at the generated queries. While not perfect, they should help nudge our embedding model in the right direction.\n",
    "\n",
    "For completeness, we'll show\n",
    "\n",
    "**(Optional) Deploy query generation model to SageMaker endpoint**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1, # number of instances\n",
    "    instance_type='ml.g4dn.xlarge', # ec2 instance type\n",
    ")\n",
    "\n",
    "# synchronous inference\n",
    "psg = passages[0]\n",
    "\n",
    "queries = predictor.predict({\n",
    "    \"inputs\": psg,\n",
    "    \"parameters\": {\n",
    "        \"max_length\": 32,\n",
    "        \"num_return_sequences\": 3,\n",
    "        \"top_p\": 0.95,\n",
    "        \"do_sample\": True\n",
    "    }\n",
    "})\n",
    "\n",
    "print(f\"Passage:\\n{psg}\\n\\nGenerated Queries:\\n\")\n",
    "for q in queries:\n",
    "    print(q[\"generated_text\"])\n",
    "\n",
    "# predictor.delete_endpoint()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, use the GenQ model to generate (query, passage) pairs for each passage. This will form the training dataset for our finetuning job.\n",
    "\n",
    "**Setup SageMaker Batch Transform Job to Generate Queries Asynchronously**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"input.jsonl\", \"w\") as f:\n",
    "    for p in passages:\n",
    "        f.write(json.dumps({\n",
    "            \"inputs\": p,\n",
    "            \"parameters\": {\n",
    "                \"max_length\": 32,\n",
    "                \"num_return_sequences\": 3,\n",
    "                \"top_p\": 0.95,\n",
    "                \"do_sample\": True\n",
    "            }}) + \"\\n\")\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "prefix = \"pubmed-finetuning\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!aws s3 cp input.jsonl s3://{bucket}/{prefix}/data/batch_input/input.jsonl"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# transform job takes about 3 hours with p3.2xlarge\n",
    "\n",
    "batch_job = huggingface_model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    strategy='SingleRecord',\n",
    "    accept='application/json',\n",
    "    assemble_with='Line',\n",
    "    output_path=f's3://{bucket}/{prefix}/data/batch_output'\n",
    ")\n",
    "\n",
    "# starts batch transform job and uses S3 data as input\n",
    "batch_job.transform(\n",
    "    data=f's3://{bucket}/{prefix}/data/batch_input/input.jsonl',\n",
    "    content_type='application/json',\n",
    "    split_type='Line',\n",
    "    join_source='Input'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Perform some light transforms on the output to turn into individual (query, passage) pairs**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# transform outputs into format suitable for training job\n",
    "local_path = 'batch_output'\n",
    "\n",
    "sagemaker.s3.S3Downloader.download(f'{batch_job.output_path}/input.jsonl.out', local_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "training_data_path = 'pubmed_training_pairs'\n",
    "\n",
    "if os.path.isdir(training_data_path) is False:\n",
    "    os.mkdir(training_data_path)\n",
    "\n",
    "pairs = []\n",
    "file_count = 0\n",
    "\n",
    "with open(f'{local_path}/input.jsonl.out', 'r') as f:\n",
    "    for r in tqdm(f.readlines()):\n",
    "        obj = json.loads(r)\n",
    "        psg = obj['inputs'].replace('\\t', ' ').replace('\\n', ' ')\n",
    "\n",
    "        for q in obj['SageMakerOutput']:\n",
    "            pairs.append(q['generated_text'].replace('\\t', ' ') + '\\t' + psg)\n",
    "\n",
    "        if len(pairs) > 1024:\n",
    "            with open(f'{training_data_path}/pairs_{file_count:04}.tsv', 'w', encoding='utf-8') as fp:\n",
    "                fp.write('\\n'.join(pairs))\n",
    "\n",
    "            file_count += 1\n",
    "            pairs = []\n",
    "\n",
    "if pairs is not None:\n",
    "    # save the final, smaller than 1024 batch\n",
    "    with open(f'{training_data_path}/pairs_{file_count:04}.tsv', 'w', encoding='utf-8') as fp:\n",
    "        fp.write('\\n'.join(pairs))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!aws s3 cp pubmed_training_pairs s3://{bucket}/{prefix}/data/training --recursive"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
